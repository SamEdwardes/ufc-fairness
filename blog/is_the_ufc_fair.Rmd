---
title: "Is UFC Judging Fair?"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(digits=2)
library(tidyverse)
library(modeldata)
library(recipes)
library(stringr)

colours <- list(blue = "#3775b7", purple = "#5737b7", green = "#37b779",
                grey = "#e9e9e9",  gold = "#b7a837", red = "#b73757")
```

```{r data, include=FALSE}
ufc <- read_csv(
  "https://github.com/SamEdwardes/ufc-data/raw/master/event_details.csv",
  col_types = cols()
)

ufc <- ufc %>%
  mutate(date = lubridate::mdy(date)) %>%
  mutate(event_name_fighters = paste0(event_name, ": ", 
                                      fighter_1_name, " vs. ", 
                                      fighter_2_name)) %>%
  mutate(event_name_fighters = stringr::str_replace_all(event_name_fighters, "_", " ")) %>%
  mutate(event_name_fighters = stringr::str_replace_all(event_name_fighters, "-", " - "))

n_fights <- nrow(ufc)
first_date <- min(ufc$date)
last_date <- max(ufc$date)

# data wrangling

ufc <- ufc %>%
  mutate(win_method_bin = case_when(
    str_detect(win_method, "DEC") ~ "Judge's Decision",
    str_detect(win_method, "TKO") ~ "Technical Knock Out (TKO)",
    str_detect(win_method, "SUB") ~ "Submission",
    TRUE ~ "Other")
  )


# ufc_fighter_details

ufc_fighter_details <- read_csv(
  "https://github.com/SamEdwardes/ufc-data/raw/master/fighter_details.csv",
  col_types = cols()
)


# summaries

df_win_types <- as_tibble(janitor::tabyl(ufc, win_method_bin))

df_win_types_judges <- ufc %>%
  filter(win_method_bin == "Judge's Decision") %>%
  janitor::tabyl(win_method) %>%
  as_tibble() 

percent_finish <- df_win_types %>% 
  filter(win_method_bin == "Technical Knock Out (TKO)" | win_method_bin == "Submission") %>%
  select(percent) %>%
  pull() %>%
  sum()

percent_judges <- df_win_types %>% 
  filter(win_method_bin == "Judge's Decision") %>%
  select(percent) %>%
  pull()

percent_unanimous <- df_win_types_judges %>%
  filter(win_method == "U-DEC") %>%
  select(percent) %>%
  pull()


# plots

fig_win_types <- ggplot(df_win_types, aes(x = win_method_bin, y = percent)) +
  geom_col(fill = colours$blue) +
  geom_label(aes(label = scales::percent(percent))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "How do UFC Fights End?",
       x = "Win Method",
       y = element_blank())

fig_judge_outcomes <- df_win_types_judges %>%
  ggplot(aes(x = win_method, y = percent)) + 
  geom_col(fill = colours$blue) +
  geom_label(aes(label = scales::percent(percent))) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Breakdown of Judging Outcomes",
       x = "Win Method",
       y = element_blank())


#' Compare two features
#'
#' @param ufc tibble 
#' @param feature_1 
#' @param feature_2 
#' @param title 
#' @param x_axis_title 
#'
#' @return ggplot
feature_comparison_plot <- function(ufc, feature_1, feature_2, title, x_axis_title, scale = FALSE) {
  
  df <- ufc %>%
    filter(win_method_bin == "Judge's Decision") %>%
    select(Winner = {{ feature_1 }}, Loser = {{ feature_2 }}, fighters = event_name_fighters) %>%
    pivot_longer(cols = c(Winner, Loser), values_to = "feature", names_to = "result")
  
  if (scale == TRUE) {
    mu <- mean(df$feature, na.rm = TRUE)
    sigma <- sd(df$feature, na.rm = TRUE)
    df$feature <- (df$feature - mu)/sigma
  }
  
  fig <- df %>%
    ggplot(aes(x = feature, y = result)) +
    geom_jitter(height = 1/4, colour = colours$blue) +
    geom_boxplot(width = 2/6, outlier.shape = NA, colour = colours$purple,
                 fill = colours$grey) +
    labs(title = title,
         x = x_axis_title,
         y = element_blank())

  fig
}
```

When a UFC match ends with a knock-out or submission there is never any doubt who the better fighter was. But only `r round(percent_finish * 100)`% of fights end with a knock-out or submission. The other `r round(percent_judges * 100)`% of fights go to the judges score cards.

```{r win types plot}
fig_win_types
```

Given that the outcome of so many fights are determined by the judges their ability to fairly score a fight is an important part of the sport. One win or loss can make or break a fighter's career. But judging is tricky business. It is frequently the subject of public scrutiny:

- [The Bleacher Report's 10 Most Controversial Judging Decisions in UFC History (2014)](https://bleacherreport.com/articles/2072171-the-10-most-controversial-judging-decisions-in-ufc-history#slide0)
- [UFC commentator Joe Rogan on the issues with judging](https://www.youtube.com/watch?v=U8ZO5k5Gykk)

How can we assess if judging in the UFC is fair? Are the judges really selecting the better performing fighter? Are they correctly applying the UFC rules to select the winner?

### Assessing fairness

To answer this question we must first understand what the rules are. How are the judges supposed to pick a winner? Then we can use statistical tools to compare what the judges are actually scoring compared to what they should be scoring.

#### What are the rules?

The system is confusing, but here are the basics. Judges score each round on a "*10-Point Must System*". The [official MMA rules](http://www.abcboxing.com/wp-content/uploads/2016/08/juding_criteriascoring_rev0816.pdf) describe scoring as follows:

> - "*A 10 –10 round in MMA is when both fighters have competed for whatever duration of time in the round and there is no difference or advantage between either fighter.*”
> - “*A 10 –9 Round in MMA is when one combatant wins the round by a close margin.*”
> - ""*A 10 –8 Round in MMA is when one fighter wins the round by a large margin.*"

The official rules also provide additional guidance on what constitutes winning:

> "*Effective Striking/Grappling shall be considered the first priority of round assessments. Effective Aggressiveness is a ‘Plan B’ and should not be considered unless the judge does not see ANY advantage in the Effective Striking/Grappling realm. Cage/Ring Control (‘Plan C’) should only be needed when ALL other criteria are 100% even for both competitors. This will be an extremely rare occurrence.*"

#### What are the judges scoring?

We can never know what was going on in a judges head. But by using the data from every UFC fight, we can identify which features are strongly correlated with winning or losing.

Logistic regression will be used to model the data. Logistic regression will assign a weight to each feature, with larger weights signalling greater importance. My hypothesis is that if the judging is fair, the weights assigned by logistic regression should be aligned with what the judges are supposed to score.

For example, based on my reading of the rules *effective striking* should be very important criteria for a judge in selecting the winner. Although there is no perfect way to report effective striking as a single number, the number of strikes landed is probably a good proxy.

The data collected has several features including:

- `Stirkes` - The number of strikes landed
- `Take-downs` - The number of successful take-downs
- `Submissions` - The number of submissions attempted
- `Pass` - The number of of guard passes

Below is an example of the data:

```{r preview of data}
ufc %>% 
  select(matches("1|2"), winner) %>%
  select(-ends_with("_url")) %>%
  head() %>%
  knitr::kable()
```

#### Assessing Fairness Visually

If the model is to work, we would expect to see that more strikes, take-downs, submissions, and passes is correlated with winning. For each feature, we can plot the relationship to visually compare.

The box plots show the median (the purple line), and the 25th and 75th percentile (the left and right hand side of the box). Each dot represents the outcome of a fight for each fighter.

```{r striking_plot, warning=FALSE}
fig_striking <- feature_comparison_plot(
  ufc,
  feature_1 = fighter_1_str,
  feature_2 = fighter_2_str,
  title = "Do Judges Reward Striking?",
  x_axis_title = "Number of Strikes Thrown",
  scale=FALSE
)

fig_pass<- feature_comparison_plot(
  ufc,
  feature_1 = fighter_1_pass,
  feature_2 = fighter_2_pass,
  title = "Do Judges Reward Passes?",
  x_axis_title = "Number of Passes",
  scale = FALSE
)

fig_td<- feature_comparison_plot(
  ufc,
  feature_1 = fighter_1_td,
  feature_2 = fighter_2_td,
  title = "Do Judges Reward Take-downs?",
  x_axis_title = "Number of Take-downs",
  scale = FALSE
)

fig_subs<- feature_comparison_plot(
  ufc,
  feature_1 = fighter_1_sub,
  feature_2 = fighter_2_sub,
  title = "Do Judges Reward Submissions?",
  x_axis_title = "Number of Submissions",
  scale = FALSE
)

gridExtra::grid.arrange(fig_striking, fig_td, fig_pass, fig_subs)
```

For striking, take downs, and passes it appears that at least visually the better performing fighter usually wins. The relationship between submissions and winning is not as clear.

#### Assessing Fairness with Logistic Regression

##### Using fighter performance features

To better understand the relationship between a fighters performance and the judges decision we can fit a logistic regression model to the data. The model will tell us which features in the data are the strongest predictor of the outcome. In this case, we have set the outcome to whether the blue fighter wins or losses. The first model will only use the features that the judges should consider: striking, take-downs, passes, and submissions.

```{r fit logistic regression}
set.seed(1993)

ufc_clean <- ufc %>%
  filter(win_method_bin == "Judge's Decision")

# shuffle the data so the winner is not always fighter_1
ufc_clean$colour_win <-sample(c("blue", "red"), nrow(ufc_clean), 
                              prob = c(0.5, 0.5), replace = TRUE)

ufc_clean <- ufc_clean %>%
  mutate(
    # fighter details
    blue_name = if_else(colour_win == "blue", fighter_1_name, fighter_2_name),
    red_name = if_else(colour_win == "red", fighter_1_name, fighter_2_name),
    blue_url = if_else(colour_win == "blue", fighter_1_url, fighter_2_url),
    red_url = if_else(colour_win == "red", fighter_1_url, fighter_2_url),
    # fight stats
    blue_str = if_else(colour_win == "blue", fighter_1_str, fighter_2_str),
    red_str = if_else(colour_win == "red", fighter_1_str, fighter_2_str),
    blue_td = if_else(colour_win == "blue", fighter_1_td, fighter_2_td),
    red_td = if_else(colour_win == "red", fighter_1_td, fighter_2_td),
    blue_sub = if_else(colour_win == "blue", fighter_1_sub, fighter_2_sub),
    red_sub = if_else(colour_win == "red", fighter_1_sub, fighter_2_sub),
    blue_pass = if_else(colour_win == "blue", fighter_1_pass, fighter_2_pass),
    red_pass = if_else(colour_win == "red", fighter_1_pass, fighter_2_pass),
    blue_win = if_else(colour_win == "blue", 1, 0),
    blue_win = as.factor(blue_win)
  )

ufc_clean <- ufc_clean %>%
  left_join(
    ufc_fighter_details %>% rename_all(list(~ paste0('blue_', .))), 
    by = c("blue_url" = "blue_fighter_url")
  ) %>% 
  left_join(
    ufc_fighter_details %>% rename_all(list(~ paste0('red_', .))), 
    by = c("red_url" = "red_fighter_url")
  )
  

# train test split
train_index <- sample(seq_len(nrow(ufc_clean)), size = nrow(ufc_clean) * 0.8)
train <- ufc_clean[train_index, ]
test <- ufc_clean[-train_index, ]

# center and scale the data
rec <- recipe(
  blue_win ~ blue_str + red_str + blue_td + red_td + blue_sub + red_sub + blue_pass + red_pass + blue_height_inches + red_height_inches + blue_weight_lbs + red_weight_lbs + blue_reach_inches + red_reach_inches,
  data = train
)

norm_trans <- rec %>%
  step_normalize(
    blue_str, red_str, blue_td, red_td, blue_sub, red_sub, blue_pass, red_pass
  )

norm_obj <- prep(norm_trans, training = train)
train_clean <- bake(norm_obj, train)
test_clean <- bake(norm_obj, test)


# fit logistic regression model
fit <- glm(
  blue_win ~ blue_str + red_str + blue_td + red_td + blue_sub + red_sub + blue_pass + red_pass,
  data = train_clean, family = "binomial"
)
```

```{r}
broom::tidy(fit) %>%
  mutate(significant = if_else(p.value < 0.05, TRUE, FALSE)) %>%
  select(term, estimate, p.value) %>%
  filter(term != "(Intercept)") %>%
  knitr::kable()
```

The table above is the output of the logistic regression model. The estimate (also known as the coefficient) tells us the effect each feature has on the odds of winning. For example, for each additional 1 standard deviation increase in the number of strikes landed by blue, they increase their **log odds** of winning by 2.28. Another way to interpret the coefficient is that for each additional 1 standard deviation increase in the number of strikes landed by blue, they increase their **odds** of winning by `r (exp(2.28) - 1) * 100`% (*(e^2.28 - 1) x 100*). The p-value tells us whether or not each coefficient is statistically significant (smaller p-values mean the model is more confident the relationship is not due to chance).

The table above can help us finally answer the question "is UFC judging fair?". The results of the logistic regression reveal that:

- As you might expect, the **most important feature is striking**. The more strikes you can land, the more likely you are to win over your opponent. Conversely, the more you get hit, the more likely you are to lose.
- The next most important feature associated with winning are **passes**, and then **take-downs**.
- **Submissions** were identified as not being statistically significant at significance level of 0.05.

We can think of the results of the logistic regression as a proxy for what judges actually care about. Further we can test the model to see how well it generalizes to the actual data.

When I fit the model above I used a random sample of 80% of the fights. We can test how well the model generalizes to the remaining 20% of the data by generating predictions. A high prediction accuracy would suggest fair judging. Or at least judging that is applied consistently.

```{r test model on test data}
train_predictions <- predict(fit, 
                            newdata = train_clean, 
                            type = "response") %>%
  round(0)

test_predictions <- predict(fit, 
                            newdata = test_clean, 
                            type = "response") %>%
  round(0)

train_accuracy <- mean(train_predictions == train_clean$blue_win, na.rm = TRUE)
test_accuracy <- mean(test_predictions == test_clean$blue_win, na.rm = TRUE)
```

On the training data, the model accurately predicts the correct winner `r train_accuracy * 100`% of the time. On the test data the model accurately predicts the correct winner `r test_accuracy * 100`% of the time.

##### Using other features

The model that was fit above used only features that the judges should use to make their decision. What will happen to the model if we add data that the judges should not be considering, but could influence their decision? For example will a fighter being taller bias the judges to reward them based on their appearance?

To test this theory we can fit the same model as before, but this time add three additional features: weight, height, and reach. If the judges are being fair, theses features should not effect the outcome of the fight.

```{r}
fit2 <- glm(
  blue_win ~ blue_str + red_str + blue_td + red_td + blue_sub + red_sub + blue_pass + red_pass + blue_height_inches + red_height_inches + blue_weight_lbs + red_weight_lbs + blue_reach_inches + red_reach_inches,
  data = train_clean, family = "binomial"
)

broom::tidy(fit2) %>%
  mutate(significant = if_else(p.value < 0.05, TRUE, FALSE)) %>%
  select(term, estimate, p.value) %>%
  filter(term != "(Intercept)") %>%
  knitr::kable()
```

Interestingly, the three "unfair" features are all considered the least important. They have a small coefficient, and a high p-value suggesting that these variables are not good predictors. This can give us some comfort that the judges are actually only considering fighter performance.

## Conclusion

So is the UFC fair? 

- Judges do reward striking, passes.
- Judges do not reward submission attempts.
- Judges do not reward height, reach, or weight.
- `r test_accuracy * 100`% of the times the judges are applying the rules consistently.

The analysis above is an attempt to answer the question, but I think the results are far from conclusive:

- Does rewarding striking, passes, and take-downs in that order align with judging rules?
- Is `r test_accuracy * 100`% consistency good enough?
- Are there other confounding variables that judges are considering that are not present in the model (fighter reputation, audience reaction, etc.)?

Personally, I think this analysis provides comfort that the judges are doing an OK job. If you land more strikes, you will probably win. If you earn more take-downs, and do a better job of passing guard you will probably win. 

But I do not know if `r test_accuracy * 100`% consistency is good enough. Is it poor judging, or something that cannot be observed in the data we have? Hopefully further analysis and the collection of additional data can help better answer this question.

---

### Reference

- You can find the complete code behind the analysis here: [https://github.com/SamEdwardes/ufc-fairness/blob/master/blog/is_the_ufc_fair.Rmd](https://github.com/SamEdwardes/ufc-fairness/blob/master/blog/is_the_ufc_fair.Rmd).
- The data used can be found here: https://github.com/SamEdwardes/ufc-data.